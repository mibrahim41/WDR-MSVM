{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8c6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import gurobipy as grb\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas\n",
    "import math\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe4f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_labels(y_in):\n",
    "    num_classes = max(y_in)\n",
    "    num_samples = len(y_in)\n",
    "    y_out_multi = np.zeros([num_samples,num_classes+1])\n",
    "    y_out_ova = np.ones([num_classes+1,num_samples])*-1\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        y_out_multi[i,y_in[i]] = 1\n",
    "        y_out_ova[y_in[i],i] = 1\n",
    "\n",
    "    return y_out_multi, y_out_ova\n",
    "\n",
    "def change_labels(perc_wrong_y,y):\n",
    "    total_samples = len(y)\n",
    "    n_classes = len(np.unique(y))\n",
    "    n_wrong = round(perc_wrong_y*total_samples)\n",
    "    idx_wrong = np.random.randint(0, high=total_samples, size=n_wrong)\n",
    "    y[idx_wrong] = np.random.randint(0, high=n_classes+1, size=n_wrong)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def gen_data(classes,imbalance,perc_wrong):\n",
    "    x_orig = scipy.io.loadmat('multi_pump_data.mat')['x_all']\n",
    "    y_orig = scipy.io.loadmat('multi_pump_data.mat')['y_all'][:,0]\n",
    "    \n",
    "    idx_shuff = np.random.permutation(len(y_orig))\n",
    "    x = x_orig[idx_shuff]\n",
    "    y = y_orig[idx_shuff]\n",
    "    \n",
    "    num_classes = np.unique(y)\n",
    "    num_test = 400\n",
    "    \n",
    "    if classes == 4:\n",
    "        if imbalance:\n",
    "            num_healthy = 170\n",
    "            num_fault = int((200 - num_healthy)/3)\n",
    "        else:\n",
    "            num_healthy = 50\n",
    "            num_fault = num_healthy\n",
    "            \n",
    "    elif classes == 7:\n",
    "        if imbalance:\n",
    "            num_healthy = 155\n",
    "            num_fault = 10\n",
    "            num_maj = 5\n",
    "        else:\n",
    "            num_healthy = 29\n",
    "            num_fault = 29\n",
    "            num_maj = 28\n",
    "            \n",
    "    x_train = x[y == 0][:num_healthy]\n",
    "    y_train = y[y == 0][:num_healthy]\n",
    "    \n",
    "    x_test = x[y == 0][num_healthy:num_healthy+num_test]\n",
    "    y_test = y[y == 0][num_healthy:num_healthy+num_test]\n",
    "            \n",
    "    i = 1\n",
    "    while i < classes:\n",
    "        if i <= 3:\n",
    "            num_active = num_fault\n",
    "        else:\n",
    "            num_active = num_maj\n",
    "            \n",
    "        x_temp_train = x[y == i][:num_active]\n",
    "        y_temp_train = y[y == i][:num_active]\n",
    "        \n",
    "        x_temp_test = x[y == i][num_active:num_active+num_test]\n",
    "        y_temp_test = y[y == i][num_active:num_active+num_test]\n",
    "        \n",
    "        x_train = np.concatenate([x_train,x_temp_train],axis=0)\n",
    "        y_train = np.concatenate([y_train,y_temp_train],axis=0)\n",
    "        \n",
    "        x_test = np.concatenate([x_test,x_temp_test],axis=0)\n",
    "        y_test = np.concatenate([y_test,y_temp_test],axis=0)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    if perc_wrong > 0:\n",
    "        y_train = change_labels(perc_wrong,y_train)\n",
    "        \n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    y_train_multi, y_train_ova = reformulate_labels(y_train)\n",
    "    y_test_multi, y_test_ova = reformulate_labels(y_test)\n",
    "    \n",
    "    x_smote = 0\n",
    "    y_smote = 0\n",
    "    y_smote_ova = 0\n",
    "    \n",
    "    if imbalance == True:\n",
    "        if classes == 4:\n",
    "            num_neighbors = 9\n",
    "        else:\n",
    "            num_neighbors = 4\n",
    "            \n",
    "        sm = SMOTE(sampling_strategy='minority',random_state=42,k_neighbors=num_neighbors)\n",
    "        x_smote, y_smote = sm.fit_resample(x_train, y_train)\n",
    "        \n",
    "        y_smote = y_smote.astype(int)\n",
    "        y_smote_multi,y_smote_ova = reformulate_labels(y_smote)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_smote, y_smote, y_train_multi, y_smote_multi, y_train_ova, y_smote_ova, x_test, y_test, y_test_multi, y_test_ova\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d2e2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_smote, y_smote, y_train_multi, y_smote_multi, y_train_ova, y_smote_ova, x_test, y_test, y_test_multi, y_test_ova = \\\n",
    "gen_data(7,True,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61e8d558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6575\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.SVC(kernel='linear')\n",
    "lin_clf.fit(x_train, y_train)\n",
    "result = lin_clf.predict(x_test)\n",
    "acc_reg = 1-np.sum(result != y_test)/len(y_test)\n",
    "print(acc_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d03fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7c7c7341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributionally Robust Multiclass SVM\n",
    "class DR_MSVM:\n",
    "    \"\"\"Distributionally robust multiclass SVM\"\"\"\n",
    "    \n",
    "    def __init__(self,param):\n",
    "        self.epsilon = param['epsilon']\n",
    "        self.kappa = param['kappa']\n",
    "        self.pnorm = param['pnorm']\n",
    "        \n",
    "        \n",
    "    def train(self,train_data):\n",
    "        \"\"\"train_data: Dictionary with 2 keys:\n",
    "            'x': N*P array of x data (N samples and P features)\n",
    "            'y': N*C array of labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "\n",
    "        row_x,col_x = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        self.num_classes = col_y\n",
    "        optimal = {}\n",
    "\n",
    "        # Creating Model\n",
    "        model = grb.Model('DRMSVM')\n",
    "        model.setParam('OutputFlag',False)\n",
    "#         model.setParam('FeasibilityTol',1e-2)\n",
    "#         model.setParam('OptimalityTol',1e-2)\n",
    "        \n",
    "\n",
    "        # Defining Decision Variables\n",
    "        var_lambda = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "        var_s = {}\n",
    "        var_M = {}\n",
    "        slack_var = {}\n",
    "        for n in range(row_x):\n",
    "            var_s[n] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "\n",
    "        for p in range(col_x):\n",
    "            for c in range(col_y):\n",
    "                var_M[c,p] = model.addVar(vtype=grb.GRB.CONTINUOUS, lb=-grb.GRB.INFINITY)\n",
    "\n",
    "        if self.pnorm == 1:\n",
    "            for p in range(col_x):\n",
    "                slack_var[p] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "    #         for c1 in range(col_y):\n",
    "    #             for c2 in range(col_y):\n",
    "    #                 slack_var[col_y*c1 + c2] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "    \n",
    "        model.update()\n",
    "\n",
    "        # Define Constraints\n",
    "        for n in range(row_x):\n",
    "            for c1 in range(col_y):\n",
    "                if y_train[n,c1] == 0:\n",
    "                    temp = 0\n",
    "                else:\n",
    "                    temp = 1\n",
    "                correct_class = np.nonzero(y_train[n])[0][0]\n",
    "                model.addConstr(\n",
    "                    grb.quicksum(var_M[c1,p]*x_train[n,p] for p in range(col_x)) - temp + 1 -  \n",
    "                    grb.quicksum(var_M[correct_class,p]*x_train[n,p] for p in range(col_x)) <= var_s[n]\n",
    "                )\n",
    "\n",
    "                for c2 in range(col_y):\n",
    "                    if c2 != correct_class:\n",
    "                        if c2 == c1:\n",
    "                            temp = 1\n",
    "                        else:\n",
    "                            temp = 0\n",
    "                        model.addConstr(\n",
    "                            grb.quicksum(var_M[c1,p]*x_train[n,p] for p in range(col_x)) - temp + 1 -  \n",
    "                            grb.quicksum(var_M[c2,p]*x_train[n,p] for p in range(col_x)) - self.kappa*var_lambda <= var_s[n]\n",
    "                        )\n",
    "\n",
    "        for c1 in range(col_y):\n",
    "            for c2 in range(col_y):\n",
    "                var_M_vec = {}\n",
    "                for p in range(col_x):\n",
    "                    var_M_vec[p] = var_M[c1,p] - var_M[c2,p]\n",
    "\n",
    "                if self.pnorm == 1:\n",
    "                    for p in range(col_x):\n",
    "                        model.addConstr(var_M_vec[p] <= slack_var[p])\n",
    "                        model.addConstr(-var_M_vec[p] <= slack_var[p])\n",
    "                    model.addConstr(grb.quicksum(slack_var[p]\n",
    "                                                 for p in range(col_x)) <= var_lambda)\n",
    "                elif self.pnorm == 2:\n",
    "                    model.addQConstr(\n",
    "                        grb.quicksum(var_M_vec[p] * var_M_vec[p]\n",
    "                                     for p in range(col_x)) <= var_lambda * var_lambda)\n",
    "\n",
    "                elif self.pnorm == float('Inf'):\n",
    "                    for p in range(col_x):\n",
    "                        model.addConstr(var_M_vec[p] <= var_lambda)\n",
    "                        model.addConstr(-var_M_vec[p] <= var_lambda)\n",
    "\n",
    "        # Define Objective Function\n",
    "        sum_var_s = grb.quicksum(var_s[n] for n in range(row_x))\n",
    "        obj = var_lambda*self.epsilon + (1/row_x)*sum_var_s\n",
    "        model.setObjective(obj,grb.GRB.MINIMIZE)\n",
    "\n",
    "        # Solve the Problem\n",
    "        model.optimize()\n",
    "\n",
    "        # Store Results\n",
    "        M_opt = np.ones([col_y,col_x])\n",
    "        for p in range(col_x):\n",
    "            for c in range(col_y):\n",
    "                M_opt[c,p] = var_M[c,p].x\n",
    "        self.M_opt = M_opt\n",
    "        results_dict = {\n",
    "            'M': M_opt,\n",
    "            'objective_value': model.ObjVal,\n",
    "            'diagnosis': model.status\n",
    "        }\n",
    "        optimal.update(results_dict)\n",
    "\n",
    "        return optimal\n",
    "    \n",
    "    \n",
    "    def test(self,test_data):\n",
    "        \"\"\"test_data: N*P array of x data (N samples and P features)\"\"\"\n",
    "        \n",
    "        x_test = test_data\n",
    "        row_x,col_x = x_test.shape\n",
    "        y_pred = np.zeros([row_x,self.num_classes])\n",
    "        \n",
    "        for n in range(row_x):\n",
    "            similarity_scores = np.matmul(self.M_opt,x_test[n])\n",
    "            prediction = np.argmax(similarity_scores)\n",
    "            y_pred[n,prediction] = 1\n",
    "            \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        incorrect_count = 0\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_class = np.nonzero(y_true[n])[0][0]\n",
    "            predicted_class = np.nonzero(y_pred[n])[0][0]\n",
    "            if true_class != predicted_class:\n",
    "                incorrect_count = incorrect_count+1\n",
    "                \n",
    "        acc = 1 - (incorrect_count/row_y)\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    def generate_conf_mat(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        true_classes = np.zeros(row_y)\n",
    "        pred_classes = np.zeros(row_y)\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_classes[n] = np.nonzero(y_true[n])[0][0]\n",
    "            pred_classes[n] = np.nonzero(y_pred[n])[0][0]\n",
    "            \n",
    "        conf_mat = sklearn.metrics.confusion_matrix(true_classes,pred_classes)\n",
    "        disp_conf_mat = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        \n",
    "        return conf_mat,disp_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a45bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Multiclass SVM\n",
    "class R_MSVM:\n",
    "    \"\"\"Distributionally robust multiclass SVM\"\"\"\n",
    "    \n",
    "    def __init__(self,param):\n",
    "        self.epsilon = param['epsilon']\n",
    "        self.pnorm = param['pnorm']\n",
    "        \n",
    "        \n",
    "    def train(self,train_data):\n",
    "        \"\"\"train_data: Dictionary with 2 keys:\n",
    "            'x': N*P array of x data (N samples and P features)\n",
    "            'y': N*C array of labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "\n",
    "        row_x,col_x = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        self.num_classes = col_y\n",
    "        optimal = {}\n",
    "\n",
    "        # Creating Model\n",
    "        model = grb.Model('DRMSVM')\n",
    "        model.setParam('OutputFlag',False)\n",
    "#         model.setParam('FeasibilityTol',1e-2)\n",
    "#         model.setParam('OptimalityTol',1e-2)\n",
    "\n",
    "        # Defining Decision Variables\n",
    "        var_lambda = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "        var_s = {}\n",
    "        var_M = {}\n",
    "        slack_var = {}\n",
    "        for n in range(row_x):\n",
    "            var_s[n] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "\n",
    "        for p in range(col_x):\n",
    "            for c in range(col_y):\n",
    "                var_M[c,p] = model.addVar(vtype=grb.GRB.CONTINUOUS, lb=-grb.GRB.INFINITY)\n",
    "\n",
    "        if self.pnorm == 1:\n",
    "            for p in range(col_x):\n",
    "                slack_var[p] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "    #         for c1 in range(col_y):\n",
    "    #             for c2 in range(col_y):\n",
    "    #                 slack_var[col_y*c1 + c2] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "    \n",
    "        model.update()\n",
    "\n",
    "        # Define Constraints\n",
    "        for n in range(row_x):\n",
    "            for c1 in range(col_y):\n",
    "                if y_train[n,c1] == 0:\n",
    "                    temp = 0\n",
    "                else:\n",
    "                    temp = 1\n",
    "                correct_class = np.nonzero(y_train[n])[0][0]\n",
    "                model.addConstr(\n",
    "                    grb.quicksum(var_M[c1,p]*x_train[n,p] for p in range(col_x)) - temp + 1 -  \n",
    "                    grb.quicksum(var_M[correct_class,p]*x_train[n,p] for p in range(col_x)) <= var_s[n]\n",
    "                )\n",
    "\n",
    "#                 for c2 in range(col_y):\n",
    "#                     if c2 != correct_class:\n",
    "#                         if c2 == c1:\n",
    "#                             temp = 0\n",
    "#                         else:\n",
    "#                             temp = 1\n",
    "#                         model.addConstr(\n",
    "#                             grb.quicksum(var_M[c1,p]*x_train[n,p] for p in range(col_x)) - temp + 1 -  \n",
    "#                             grb.quicksum(var_M[c2,p]*x_train[n,p] for p in range(col_x)) - self.kappa*var_lambda <= var_s[n]\n",
    "#                         )\n",
    "\n",
    "        for c1 in range(col_y):\n",
    "            for c2 in range(col_y):\n",
    "                var_M_vec = {}\n",
    "                for p in range(col_x):\n",
    "                    var_M_vec[p] = var_M[c1,p] - var_M[c2,p]\n",
    "\n",
    "                if self.pnorm == 1:\n",
    "                    for p in range(col_x):\n",
    "                        model.addConstr(var_M_vec[p] <= slack_var[p])\n",
    "                        model.addConstr(-var_M_vec[p] <= slack_var[p])\n",
    "                    model.addConstr(grb.quicksum(slack_var[p]\n",
    "                                                 for p in range(col_x)) <= var_lambda)\n",
    "                elif self.pnorm == 2:\n",
    "                    model.addQConstr(\n",
    "                        grb.quicksum(var_M_vec[p] * var_M_vec[p]\n",
    "                                     for p in range(col_x)) <= var_lambda * var_lambda)\n",
    "\n",
    "                elif self.pnorm == float('Inf'):\n",
    "                    for p in range(col_x):\n",
    "                        model.addConstr(var_M_vec[p] <= var_lambda)\n",
    "                        model.addConstr(-var_M_vec[p] <= var_lambda)\n",
    "\n",
    "        # Define Objective Function\n",
    "        sum_var_s = grb.quicksum(var_s[n] for n in range(row_x))\n",
    "        obj = var_lambda*self.epsilon + (1/row_x)*sum_var_s\n",
    "        model.setObjective(obj,grb.GRB.MINIMIZE)\n",
    "\n",
    "        # Solve the Problem\n",
    "        model.optimize()\n",
    "\n",
    "        # Store Results\n",
    "        M_opt = np.ones([col_y,col_x])\n",
    "        for p in range(col_x):\n",
    "            for c in range(col_y):\n",
    "                M_opt[c,p] = var_M[c,p].x\n",
    "        self.M_opt = M_opt\n",
    "        results_dict = {\n",
    "            'M': M_opt,\n",
    "            'objective_value': model.ObjVal,\n",
    "            'diagnosis': model.status\n",
    "        }\n",
    "        optimal.update(results_dict)\n",
    "\n",
    "        return optimal\n",
    "    \n",
    "    \n",
    "    def test(self,test_data):\n",
    "        \"\"\"test_data: N*P array of x data (N samples and P features)\"\"\"\n",
    "        \n",
    "        x_test = test_data\n",
    "        row_x,col_x = x_test.shape\n",
    "        y_pred = np.zeros([row_x,self.num_classes])\n",
    "        \n",
    "        for n in range(row_x):\n",
    "            similarity_scores = np.matmul(self.M_opt,x_test[n])\n",
    "            prediction = np.argmax(similarity_scores)\n",
    "            y_pred[n,prediction] = 1\n",
    "            \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        incorrect_count = 0\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_class = np.nonzero(y_true[n])[0][0]\n",
    "            predicted_class = np.nonzero(y_pred[n])[0][0]\n",
    "            if true_class != predicted_class:\n",
    "                incorrect_count = incorrect_count+1\n",
    "                \n",
    "        acc = 1 - (incorrect_count/row_y)\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    def generate_conf_mat(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        true_classes = np.zeros(row_y)\n",
    "        pred_classes = np.zeros(row_y)\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_classes[n] = np.nonzero(y_true[n])[0][0]\n",
    "            pred_classes[n] = np.nonzero(y_pred[n])[0][0]\n",
    "            \n",
    "        conf_mat = sklearn.metrics.confusion_matrix(true_classes,pred_classes)\n",
    "        disp_conf_mat = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        \n",
    "        return conf_mat,disp_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributionally Robust One-vs-All\n",
    "class DR_OVA:\n",
    "    \"\"\" One-Vs-All distributionally robust binary SVM\"\"\"\n",
    "    \n",
    "    def __init__(self,param):\n",
    "        self.epsilon = param['epsilon']\n",
    "        self.kappa = param['kappa']\n",
    "        self.pnorm = param['pnorm']\n",
    "        \n",
    "    def train(self,train_data):\n",
    "        \"\"\"train_data: Dictionary with 2 keys:\n",
    "            'x': N*P array of x data (N samples and P features)\n",
    "            'y': C*N array of labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "\n",
    "        row_x,col_x = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        self.num_classes = row_y\n",
    "        \n",
    "        optimal_all_classes = {}\n",
    "        self.w_opt_all_classes = np.zeros([row_y,col_x])\n",
    "        for c in range(self.num_classes):\n",
    "            y_c = y_train[c]\n",
    "            data_c = {'x':x_train, 'y':y_c}\n",
    "            opt_dr_svm_c = self.dist_rob_svm_without_support(data_c)\n",
    "            optimal_all_classes[\"optimal_\" + str(c)] = opt_dr_svm_c\n",
    "            self.w_opt_all_classes[c,:] = opt_dr_svm_c[\"w\"]\n",
    "            \n",
    "        return optimal_all_classes    \n",
    "        \n",
    "    def dist_rob_svm_without_support(self,data):\n",
    "        \"\"\" distributionally robust SVM without support information \"\"\"\n",
    "        \n",
    "        x_train = data['x']\n",
    "        y_train = data['y'].flatten()\n",
    "\n",
    "        row, col = x_train.shape\n",
    "        optimal = {}\n",
    "\n",
    "        # Step 0: create model\n",
    "        model = grb.Model('DRSVM_without_support')\n",
    "        model.setParam('OutputFlag', False)\n",
    "#         model.setParam('FeasibilityTol',1e-2)\n",
    "#         model.setParam('OptimalityTol',1e-2)\n",
    "\n",
    "        # Step 1: define decision variables\n",
    "        var_lambda = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "        var_s = {}\n",
    "        var_w = {}\n",
    "        slack_var = {}\n",
    "        for i in range(row):\n",
    "            var_s[i] = model.addVar(vtype=grb.GRB.CONTINUOUS,)\n",
    "        for j in range(col):\n",
    "            var_w[j] = model.addVar(\n",
    "                vtype=grb.GRB.CONTINUOUS, lb=-grb.GRB.INFINITY)\n",
    "            if self.pnorm == 1:\n",
    "                slack_var[j] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "\n",
    "        # Step 2: integerate variables\n",
    "        model.update()\n",
    "\n",
    "        # Step 3: define constraints\n",
    "        for i in range(row):\n",
    "            model.addConstr(\n",
    "                1 - y_train[i] * grb.quicksum(var_w[j] * x_train[i, j]\n",
    "                                              for j in range(col)) <= var_s[i])\n",
    "            model.addConstr(\n",
    "                1 + y_train[i] * grb.quicksum(var_w[j] * x_train[i, j]\n",
    "                                              for j in range(col)) -\n",
    "                self.kappa * var_lambda <= var_s[i])\n",
    "\n",
    "        if self.pnorm == 1:\n",
    "            for j in range(col):\n",
    "                model.addConstr(var_w[j] <= slack_var[j])\n",
    "                model.addConstr(-var_w[j] <= slack_var[j])\n",
    "            model.addConstr(grb.quicksum(slack_var[j]\n",
    "                                         for j in range(col)) <= var_lambda)\n",
    "        elif self.pnorm == 2:\n",
    "            model.addQConstr(\n",
    "                grb.quicksum(var_w[j] * var_w[j]\n",
    "                             for j in range(col)) <= var_lambda*var_lambda)\n",
    "\n",
    "        elif self.pnorm == float('Inf'):\n",
    "            for j in range(col):\n",
    "                model.addConstr(var_w[j] <= var_lambda)\n",
    "                model.addConstr(-var_w[j] <= var_lambda)\n",
    "\n",
    "        # Step 4: define objective value\n",
    "        sum_var_s = grb.quicksum(var_s[i] for i in range(row))\n",
    "        obj = var_lambda*self.epsilon + (1/row)*sum_var_s\n",
    "        model.setObjective(obj, grb.GRB.MINIMIZE)\n",
    "\n",
    "        # Step 5: solve the problem\n",
    "        model.optimize()\n",
    "\n",
    "        # Step 6: store results\n",
    "        w_opt = np.array([var_w[i].x for i in range(col)])\n",
    "        tmp = {'w': w_opt,'objective': model.ObjVal,'diagnosis': model.status}\n",
    "        optimal.update(tmp)\n",
    "\n",
    "        return optimal\n",
    "    \n",
    "    def test(self,test_data):\n",
    "        \"\"\"test_data: N*P array of x data (N samples and P features)\"\"\"\n",
    "        \n",
    "        x_test = test_data\n",
    "        row_x,col_x = x_test.shape\n",
    "        y_pred = np.zeros([row_x])\n",
    "        \n",
    "        for n in range(row_x):\n",
    "            scores = np.ones([self.num_classes])*-1e10\n",
    "            for c in range(self.num_classes):\n",
    "                w_c = self.w_opt_all_classes[c]\n",
    "                test_sample = x_test[n]\n",
    "                pred_c = np.sum(w_c*test_sample)\n",
    "                scores[c] = pred_c\n",
    "            y_pred[n] = np.argmax(scores)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*1 array of true labels\n",
    "           y_pred: N*1 array of predicted labels\"\"\"\n",
    "        \n",
    "        acc = 1-np.sum(y_pred != y_true)/len(y_true)\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    def generate_conf_mat(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*1 array of true labels\n",
    "           y_pred: N*1 array of predicted labels\"\"\"\n",
    "            \n",
    "        conf_mat = sklearn.metrics.confusion_matrix(y_true,y_pred)\n",
    "        disp_conf_mat = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        \n",
    "        return conf_mat,disp_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d37a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Distributionally Robust Multiclass SVM\n",
    "class kDR_MSVM:\n",
    "    \n",
    "    def __init__(self,param):\n",
    "        self.epsilon = param['epsilon']\n",
    "        self.kappa = param['kappa']\n",
    "        self.kernel = param['kernel']\n",
    "        if self.kernel == \"rbf\" or self.kernel == \"laplacian\":\n",
    "            self.gamma = param[\"gamma\"]\n",
    "        elif self.kernel == \"poly\":\n",
    "            self.gamma = param[\"gamma\"]\n",
    "            self.d = param[\"d\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute_kernel_entry(self,x1,x2):\n",
    "        if self.kernel == \"rbf\":\n",
    "            return np.exp(-self.gamma*(np.linalg.norm(x1-x2, ord=2)**2))\n",
    "        elif self.kernel == \"laplacian\":\n",
    "            return np.exp(-self.gamma*np.linalg.norm(x1-x2, ord=1))\n",
    "        elif self.kernel == \"poly\":\n",
    "            return (self.gamma*np.sum(x1*x2) + 1)**self.d\n",
    "        \n",
    "        \n",
    "    def train(self,train_data):\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "        \n",
    "        row_x,col_x = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        self.num_classes = col_y\n",
    "        \n",
    "        if self.gamma == 'Auto':\n",
    "            self.gamma = 1/col_x\n",
    "        \n",
    "        k_train = np.zeros([row_x,row_x])\n",
    "        for n1 in range(row_x):\n",
    "            for n2 in range(row_x):\n",
    "                k_train[n1,n2] = self.compute_kernel_entry(x_train[n1,:],x_train[n2,:])\n",
    "                \n",
    "        optimal = {}\n",
    "        \n",
    "        # Creating Model\n",
    "        model = grb.Model('kDRMSVM')\n",
    "        model.setParam('OutputFlag',False)\n",
    "#         model.setParam('FeasibilityTol',1e-2)\n",
    "#         model.setParam('OptimalityTol',1e-2)\n",
    "#         model.setParam('NonConvex', 2)\n",
    "        \n",
    "        # Defining Decision Variables\n",
    "        var_lambda = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "        var_s = {}\n",
    "        var_A = {}\n",
    "        for n in range(row_x):\n",
    "            var_s[n] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "            for c in range(col_y):\n",
    "                var_A[c,n] = model.addVar(vtype=grb.GRB.CONTINUOUS, lb=-grb.GRB.INFINITY)\n",
    "                \n",
    "                \n",
    "        model.update()\n",
    "        \n",
    "        # Defining Constraints\n",
    "        for n in range(row_x):\n",
    "            for c1 in range(col_y):\n",
    "                if y_train[n,c1] == 0:\n",
    "                    temp = 0\n",
    "                else:\n",
    "                    temp = 1\n",
    "                correct_class = np.nonzero(y_train[n])[0][0]\n",
    "                model.addConstr(\n",
    "                    grb.quicksum(var_A[c1,j]*k_train[n,j] for j in range(row_x)) - temp + 1 -\n",
    "                    grb.quicksum(var_A[correct_class,j]*k_train[n,j] for j in range(row_x)) <= var_s[n]\n",
    "                )\n",
    "                \n",
    "                for c2 in range(col_y):\n",
    "                    if c2 != correct_class:\n",
    "                        if c2 == c1:\n",
    "                            temp = 1\n",
    "                        else:\n",
    "                            temp = 0\n",
    "                        model.addConstr(\n",
    "                            grb.quicksum(var_A[c1,j]*k_train[n,j] for j in range(row_x)) - temp + 1 -\n",
    "                            grb.quicksum(var_A[c2,j]*k_train[n,j] for j in range(row_x)) - self.kappa*var_lambda <= var_s[n]\n",
    "                        )\n",
    "                        \n",
    "        for c1 in range(col_y):\n",
    "            for c2 in range(col_y):\n",
    "                if c2 > c1:\n",
    "                    model.addQConstr(\n",
    "                        grb.quicksum(var_A[c1,n1]*k_train[n1,n2]*var_A[c1,n2]\n",
    "                                    for n1 in range(row_x) \n",
    "                                    for n2 in range(row_x)) +\n",
    "                        grb.quicksum(var_A[c2,n1]*k_train[n1,n2]*var_A[c2,n2]\n",
    "                                    for n1 in range(row_x)\n",
    "                                    for n2 in range(row_x)) <= var_lambda*var_lambda)\n",
    "                \n",
    "        # Define Objective Function\n",
    "        sum_var_s = grb.quicksum(var_s[n] for n in range(row_x))\n",
    "        obj = var_lambda*self.epsilon + (1/row_x)*sum_var_s\n",
    "        model.setObjective(obj,grb.GRB.MINIMIZE)\n",
    "        \n",
    "        # Solve the Problem\n",
    "        model.optimize()\n",
    "        \n",
    "        # Store Results\n",
    "        A_opt = np.ones([col_y,row_x])\n",
    "        for n in range(row_x):\n",
    "            for c in range(col_y):\n",
    "                A_opt[c,n] = var_A[c,n].x\n",
    "        \n",
    "        self.A_opt = A_opt\n",
    "        results_dict = {\n",
    "            'A': A_opt,\n",
    "            'objective_value': model.ObjVal,\n",
    "            'diagnosis': model.status\n",
    "        }\n",
    "        optimal.update(results_dict)\n",
    "        \n",
    "        return optimal\n",
    "    \n",
    "    def test(self,test_data,train_data):\n",
    "        x_test = test_data\n",
    "        row_x,col_x = x_test.shape\n",
    "        y_pred = np.zeros([row_x,self.num_classes])\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "        row_x_train,col_x_train = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        \n",
    "        for n1 in range(row_x):\n",
    "            similarity_scores = np.zeros([col_y])\n",
    "            for c in range(col_y):\n",
    "                k_vec = np.zeros([row_x_train])\n",
    "                for n2 in range(row_x_train):\n",
    "                    k_vec[n2] = self.compute_kernel_entry(x_test[n1,:],x_train[n2,:])\n",
    "                similarity_scores[c] = np.sum(self.A_opt[c,:]*k_vec)\n",
    "                \n",
    "            prediction = np.argmax(similarity_scores)\n",
    "            y_pred[n1,prediction] = 1\n",
    "            \n",
    "        return y_pred\n",
    "        \n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        incorrect_count = 0\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_class = np.nonzero(y_true[n])[0][0]\n",
    "            predicted_class = np.nonzero(y_pred[n])[0][0]\n",
    "            if true_class != predicted_class:\n",
    "                incorrect_count = incorrect_count+1\n",
    "                \n",
    "        acc = 1 - (incorrect_count/row_y)\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    def generate_conf_mat(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        true_classes = np.zeros(row_y)\n",
    "        pred_classes = np.zeros(row_y)\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_classes[n] = np.nonzero(y_true[n])[0][0]\n",
    "            pred_classes[n] = np.nonzero(y_pred[n])[0][0]\n",
    "            \n",
    "        conf_mat = sklearn.metrics.confusion_matrix(true_classes,pred_classes)\n",
    "        disp_conf_mat = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        \n",
    "        return conf_mat,disp_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Regularized Multiclass SVM\n",
    "class kR_MSVM:\n",
    "    \n",
    "    def __init__(self,param):\n",
    "        self.epsilon = param['epsilon']\n",
    "        self.kernel = param['kernel']\n",
    "        if self.kernel == \"rbf\" or self.kernel == \"laplacian\":\n",
    "            self.gamma = param[\"gamma\"]\n",
    "        elif self.kernel == \"poly\":\n",
    "            self.gamma = param[\"gamma\"]\n",
    "            self.d = param[\"d\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute_kernel_entry(self,x1,x2):\n",
    "        if self.kernel == \"rbf\":\n",
    "            return np.exp(-self.gamma*(np.linalg.norm(x1-x2, ord=2)**2))\n",
    "        elif self.kernel == \"laplacian\":\n",
    "            return np.exp(-self.gamma*np.linalg.norm(x1-x2, ord=1))\n",
    "        elif self.kernel == \"poly\":\n",
    "            return (self.gamma*np.sum(x1*x2) + 1)**self.d\n",
    "        \n",
    "        \n",
    "    def train(self,train_data):\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "        \n",
    "        row_x,col_x = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        self.num_classes = col_y\n",
    "        \n",
    "        if self.gamma == 'Auto':\n",
    "            self.gamma = 1/col_x\n",
    "        \n",
    "        k_train = np.zeros([row_x,row_x])\n",
    "        for n1 in range(row_x):\n",
    "            for n2 in range(row_x):\n",
    "                k_train[n1,n2] = self.compute_kernel_entry(x_train[n1,:],x_train[n2,:])\n",
    "                \n",
    "        optimal = {}\n",
    "        \n",
    "        # Creating Model\n",
    "        model = grb.Model('kDRMSVM')\n",
    "        model.setParam('OutputFlag',False)\n",
    "#         model.setParam('FeasibilityTol',1e-2)\n",
    "#         model.setParam('OptimalityTol',1e-2)\n",
    "#         model.setParam('NonConvex', 2)\n",
    "        \n",
    "        # Defining Decision Variables\n",
    "        var_lambda = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "        var_s = {}\n",
    "        var_A = {}\n",
    "        for n in range(row_x):\n",
    "            var_s[n] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "            for c in range(col_y):\n",
    "                var_A[c,n] = model.addVar(vtype=grb.GRB.CONTINUOUS, lb=-grb.GRB.INFINITY)\n",
    "                \n",
    "                \n",
    "        model.update()\n",
    "        \n",
    "        # Defining Constraints\n",
    "        for n in range(row_x):\n",
    "            for c1 in range(col_y):\n",
    "                if y_train[n,c1] == 0:\n",
    "                    temp = 0\n",
    "                else:\n",
    "                    temp = 1\n",
    "                correct_class = np.nonzero(y_train[n])[0][0]\n",
    "                model.addConstr(\n",
    "                    grb.quicksum(var_A[c1,j]*k_train[n,j] for j in range(row_x)) - temp + 1 -\n",
    "                    grb.quicksum(var_A[correct_class,j]*k_train[n,j] for j in range(row_x)) <= var_s[n]\n",
    "                )\n",
    "                \n",
    "#                 for c2 in range(col_y):\n",
    "#                     if c2 != correct_class:\n",
    "#                         if c2 == c1:\n",
    "#                             temp = 1\n",
    "#                         else:\n",
    "#                             temp = 0\n",
    "#                         model.addConstr(\n",
    "#                             grb.quicksum(var_A[c1,j]*k_train[n,j] for j in range(row_x)) - temp + 1 -\n",
    "#                             grb.quicksum(var_A[c2,j]*k_train[n,j] for j in range(row_x)) - self.kappa*var_lambda <= var_s[n]\n",
    "#                         )\n",
    "                        \n",
    "        for c1 in range(col_y):\n",
    "            for c2 in range(col_y):\n",
    "                if c2 > c1:\n",
    "                    model.addQConstr(\n",
    "                        grb.quicksum(var_A[c1,n1]*k_train[n1,n2]*var_A[c1,n2]\n",
    "                                    for n1 in range(row_x) \n",
    "                                    for n2 in range(row_x)) +\n",
    "                        grb.quicksum(var_A[c2,n1]*k_train[n1,n2]*var_A[c2,n2]\n",
    "                                    for n1 in range(row_x)\n",
    "                                    for n2 in range(row_x)) <= var_lambda*var_lambda)\n",
    "                \n",
    "        # Define Objective Function\n",
    "        sum_var_s = grb.quicksum(var_s[n] for n in range(row_x))\n",
    "        obj = var_lambda*self.epsilon + (1/row_x)*sum_var_s\n",
    "        model.setObjective(obj,grb.GRB.MINIMIZE)\n",
    "        \n",
    "        # Solve the Problem\n",
    "        model.optimize()\n",
    "        \n",
    "        # Store Results\n",
    "        A_opt = np.ones([col_y,row_x])\n",
    "        for n in range(row_x):\n",
    "            for c in range(col_y):\n",
    "                A_opt[c,n] = var_A[c,n].x\n",
    "        \n",
    "        self.A_opt = A_opt\n",
    "        results_dict = {\n",
    "            'A': A_opt,\n",
    "            'objective_value': model.ObjVal,\n",
    "            'diagnosis': model.status\n",
    "        }\n",
    "        optimal.update(results_dict)\n",
    "        \n",
    "        return optimal\n",
    "    \n",
    "    def test(self,test_data,train_data):\n",
    "        x_test = test_data\n",
    "        row_x,col_x = x_test.shape\n",
    "        y_pred = np.zeros([row_x,self.num_classes])\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "        row_x_train,col_x_train = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        \n",
    "        for n1 in range(row_x):\n",
    "            similarity_scores = np.zeros([col_y])\n",
    "            for c in range(col_y):\n",
    "                k_vec = np.zeros([row_x_train])\n",
    "                for n2 in range(row_x_train):\n",
    "                    k_vec[n2] = self.compute_kernel_entry(x_test[n1,:],x_train[n2,:])\n",
    "                similarity_scores[c] = np.sum(self.A_opt[c,:]*k_vec)\n",
    "                \n",
    "            prediction = np.argmax(similarity_scores)\n",
    "            y_pred[n1,prediction] = 1\n",
    "            \n",
    "        return y_pred\n",
    "        \n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        incorrect_count = 0\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_class = np.nonzero(y_true[n])[0][0]\n",
    "            predicted_class = np.nonzero(y_pred[n])[0][0]\n",
    "            if true_class != predicted_class:\n",
    "                incorrect_count = incorrect_count+1\n",
    "                \n",
    "        acc = 1 - (incorrect_count/row_y)\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    def generate_conf_mat(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*C array of true labels (N samples and C classes)\n",
    "           y_pred: N*C array of predicted labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        row_y,col_y = y_true.shape\n",
    "        true_classes = np.zeros(row_y)\n",
    "        pred_classes = np.zeros(row_y)\n",
    "        \n",
    "        for n in range(row_y):\n",
    "            true_classes[n] = np.nonzero(y_true[n])[0][0]\n",
    "            pred_classes[n] = np.nonzero(y_pred[n])[0][0]\n",
    "            \n",
    "        conf_mat = sklearn.metrics.confusion_matrix(true_classes,pred_classes)\n",
    "        disp_conf_mat = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        \n",
    "        return conf_mat,disp_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Distributionally Robust One-vs-All SVM\n",
    "class kDR_OVA:\n",
    "    \"\"\" One-Vs-All distributionally robust binary SVM\"\"\"\n",
    "    \n",
    "    def __init__(self,param):\n",
    "        self.epsilon = param['epsilon']\n",
    "        self.kappa = param['kappa']\n",
    "        self.kernel = param['kernel']\n",
    "        if self.kernel == \"rbf\" or self.kernel == \"laplacian\":\n",
    "            self.gamma = param[\"gamma\"]\n",
    "        elif self.kernel == \"poly\":\n",
    "            self.gamma = param[\"gamma\"]\n",
    "            self.d = param[\"d\"]\n",
    "\n",
    "            \n",
    "    def compute_kernel_entry(self,x1,x2):\n",
    "        if self.kernel == \"rbf\":\n",
    "            return np.exp(-self.gamma*(np.linalg.norm(x1-x2, ord=2)**2))\n",
    "        elif self.kernel == \"laplacian\":\n",
    "            return np.exp(-self.gamma*np.linalg.norm(x1-x2, ord=1))\n",
    "        elif self.kernel == \"poly\":\n",
    "            return (self.gamma*np.sum(x1*x2) + 1)**self.d\n",
    "        \n",
    "    def train(self,train_data):\n",
    "        \"\"\"train_data: Dictionary with 2 keys:\n",
    "            'x': N*P array of x data (N samples and P features)\n",
    "            'y': C*N array of labels (N samples and C classes)\"\"\"\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "\n",
    "        row_x,col_x = x_train.shape\n",
    "        row_y,col_y = y_train.shape\n",
    "        self.num_classes = row_y\n",
    "        \n",
    "        if self.gamma == 'Auto':\n",
    "            self.gamma = 1/col_x\n",
    "        \n",
    "        k_train = np.zeros([row_x,row_x])\n",
    "        for n1 in range(row_x):\n",
    "            for n2 in range(row_x):\n",
    "                k_train[n1,n2] = self.compute_kernel_entry(x_train[n1,:],x_train[n2,:])\n",
    "                \n",
    "        optimal_all_classes = {}\n",
    "        self.alpha_opt_all_classes = np.zeros([row_y,row_x])\n",
    "        \n",
    "        for c in range(self.num_classes):\n",
    "            y_c = y_train[c]\n",
    "            data_c = {'k':k_train, 'y':y_c}\n",
    "            opt_dr_svm_c = self.kernel_dist_rob_svm_without_support(data_c)\n",
    "            optimal_all_classes[\"optimal_\" + str(c)] = opt_dr_svm_c\n",
    "            self.alpha_opt_all_classes[c,:] = opt_dr_svm_c[\"alpha\"]\n",
    "            \n",
    "        return optimal_all_classes\n",
    "    \n",
    "    def kernel_dist_rob_svm_without_support(self, data):\n",
    "        \"\"\" kernelized distributionally robust SVM \"\"\"\n",
    "        k_train = data['k']\n",
    "        y_train = data['y'].flatten()\n",
    "\n",
    "        row = k_train.shape[0]\n",
    "        optimal = {}\n",
    "\n",
    "        # Step 0: create model\n",
    "        model = grb.Model('Ker_DRSVM')\n",
    "        model.setParam('OutputFlag', False)\n",
    "#         model.setParam('FeasibilityTol',1e-2)\n",
    "#         model.setParam('OptimalityTol',1e-2)\n",
    "#         model.setParam('NonConvex', 2)\n",
    "\n",
    "        # Step 1: define decision variables\n",
    "        var_lambda = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "        var_s = {}\n",
    "        var_alpha = {}\n",
    "        for i in range(row):\n",
    "            var_s[i] = model.addVar(vtype=grb.GRB.CONTINUOUS)\n",
    "            var_alpha[i] = model.addVar(\n",
    "                vtype=grb.GRB.CONTINUOUS, lb=-grb.GRB.INFINITY)\n",
    "\n",
    "        # Step 2: integerate variables\n",
    "        model.update()\n",
    "\n",
    "        # Step 3: define constraints\n",
    "        for i in range(row):\n",
    "            model.addConstr(\n",
    "                1 - y_train[i] * grb.quicksum(var_alpha[k] * k_train[k, i]\n",
    "                                              for k in range(row)) <= var_s[i])\n",
    "            model.addConstr(\n",
    "                1 + y_train[i] * grb.quicksum(var_alpha[k] * k_train[k, i]\n",
    "                                              for k in range(row)) -\n",
    "                self.kappa * var_lambda <= var_s[i])\n",
    "        model.addQConstr(\n",
    "            grb.quicksum(var_alpha[k1] * k_train[k1, k2] * var_alpha[k2]\n",
    "                         for k1 in range(row)\n",
    "                         for k2 in range(row)) <= var_lambda * var_lambda)\n",
    "\n",
    "        # Step 4: define objective value\n",
    "        sum_var_s = grb.quicksum(var_s[i] for i in range(row))\n",
    "        obj = var_lambda*self.epsilon + (1/row)*sum_var_s\n",
    "        model.setObjective(obj, grb.GRB.MINIMIZE)\n",
    "\n",
    "        # Step 5: solve the problem\n",
    "        model.optimize()\n",
    "\n",
    "        # Step 6: store results\n",
    "        alpha_opt = np.array([var_alpha[i].x for i in range(row)])\n",
    "        tmp = {'alpha': alpha_opt,'objective': model.ObjVal,'diagnosis': model.status}\n",
    "        optimal.update(tmp)\n",
    "\n",
    "        return optimal\n",
    "    \n",
    "    def test(self,test_data,train_data):\n",
    "        \"\"\"test_data: N*P array of x data (N samples and P features)\"\"\"\n",
    "        \n",
    "        x_test = test_data\n",
    "        row_x,col_x = x_test.shape\n",
    "        y_pred = np.zeros([row_x])\n",
    "        \n",
    "        x_train = train_data['x']\n",
    "        y_train = train_data['y']\n",
    "        row_x_train,col_x_train = x_train.shape\n",
    "        \n",
    "        for n1 in range(row_x):\n",
    "            scores = np.ones([self.num_classes])*-1e10\n",
    "            for c in range(self.num_classes):\n",
    "                alpha_c = self.alpha_opt_all_classes[c]\n",
    "                test_sample = x_test[n1]\n",
    "                k_vec = np.zeros([row_x_train])\n",
    "                for n2 in range(row_x_train):\n",
    "                    k_vec[n2] = self.compute_kernel_entry(x_test[n1,:],x_train[n2,:])\n",
    "                \n",
    "                pred_c = np.sum(alpha_c*k_vec)\n",
    "                scores[c] = pred_c\n",
    "            y_pred[n1] = np.argmax(scores)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*1 array of true labels\n",
    "           y_pred: N*1 array of predicted labels\"\"\"\n",
    "        \n",
    "        acc = 1-np.sum(y_pred != y_true)/len(y_true)\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    def generate_conf_mat(self,y_true,y_pred):\n",
    "        \"\"\"y_true: N*1 array of true labels\n",
    "           y_pred: N*1 array of predicted labels\"\"\"\n",
    "            \n",
    "        conf_mat = sklearn.metrics.confusion_matrix(y_true,y_pred)\n",
    "        disp_conf_mat = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        \n",
    "        return conf_mat,disp_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f836a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_vec = [1e-5,1e-4,1e-3,1e-2,1e-1]\n",
    "kappa_vec = [0.25,0.5,0.75,1]\n",
    "gamma = 1/17\n",
    "\n",
    "acc_tensor_multi = np.zeros(len(epsilon_vec),len(kappa_vec),2)\n",
    "acc_tensor_multi_ker_rbf = np.zeros(len(epsilon_vec),len(kappa_vec),2)\n",
    "\n",
    "acc_tensor_ova = np.zeros(len(epsilon_vec),len(kappa_vec),2)\n",
    "acc_tensor_ova_ker_rbf = np.zeros(len(epsilon_vec),len(kappa_vec),2)\n",
    "\n",
    "acc_tensor_Rmulti = np.zeros(len(epsilon_vec),2)\n",
    "acc_tensor_Rmulti_ker_rbf = np.zeros(len(epsilon_vec),2)\n",
    "\n",
    "x_train, y_train, x_smote, y_smote, y_train_multi, y_smote_multi, y_train_ova, y_smote_ova, x_test, y_test, y_test_multi, y_test_ova = \\\n",
    "gen_data(4,True,0)\n",
    "\n",
    "train_data_multi = {'x': x_train, 'y': y_train_multi}\n",
    "train_data_ova = {'x': x_train, 'y': y_train_ova}\n",
    "\n",
    "train_data_multi_s = {'x': x_smote, 'y': y_smote_multi}\n",
    "train_data_ova_s = {'x': x_smote, 'y': y_smote_ova}\n",
    "\n",
    "for i in range(epsilon_vec):\n",
    "    param_multi_reg = {'epsilon': epsilon_vec[i], 'pnorm':float('Inf')}\n",
    "    param_multi_reg_ker_rbf = {'epsilon': epsilon_vec[i], 'pnorm':float('Inf'), 'kernel': 'rbf', 'gamma':'Auto'}\n",
    "    \n",
    "    classifier_multi_reg = R_MSVM(param_multi_reg)\n",
    "    optimal_multi_reg = classifier_multi_reg.train(train_data_multi)\n",
    "    y_pred_multi_reg = classifier_multi_reg.test(x_test)\n",
    "    acc_multi_reg = classifier_multi_reg.evaluate_accuracy(y_test_multi,y_pred_multi_reg)\n",
    "    acc_tensor_Rmulti[i,0] = acc_multi_reg\n",
    "    \n",
    "    classifier_multi_reg = R_MSVM(param_multi_reg)\n",
    "    optimal_multi_reg = classifier_multi_reg.train(train_data_multi_s)\n",
    "    y_pred_multi_reg = classifier_multi_reg.test(x_test)\n",
    "    acc_multi_reg = classifier_multi_reg.evaluate_accuracy(y_test_multi,y_pred_multi_reg)\n",
    "    acc_tensor_Rmulti[i,1] = acc_multi_reg\n",
    "    \n",
    "    classifier_multi_reg_ker_rbf = kR_MSVM(param_multi_reg_ker_rbf)\n",
    "    optimal_multi_reg_ker_rbf = classifier_multi_reg_ker_rbf.train(train_data_multi)\n",
    "    y_pred_multi_reg_ker_rbf = classifier_multi_reg_ker_rbf.test(x_test,train_data_multi)\n",
    "    acc_multi_reg_ker_rbf = classifier_multi_reg_ker_rbf.evaluate_accuracy(y_test_multi,y_pred_multi_reg_ker_rbf)\n",
    "    acc_tensor_Rmulti_ker_rbf[i,0] = acc_multi_reg_ker_rbf\n",
    "    \n",
    "    classifier_multi_reg_ker_rbf = kR_MSVM(param_multi_reg_ker_rbf)\n",
    "    optimal_multi_reg_ker_rbf = classifier_multi_reg_ker_rbf.train(train_data_multi_s)\n",
    "    y_pred_multi_reg_ker_rbf = classifier_multi_reg_ker_rbf.test(x_test,train_data_multi)\n",
    "    acc_multi_reg_ker_rbf = classifier_multi_reg_ker_rbf.evaluate_accuracy(y_test_multi,y_pred_multi_reg_ker_rbf)\n",
    "    acc_tensor_Rmulti_ker_rbf[i,1] = acc_multi_reg_ker_rbf\n",
    "    \n",
    "    \n",
    "    for j in range(kappa_vec):\n",
    "        param_multi = {'epsilon': epsilon_vec[i], 'kappa': kappa_vec[j], 'pnorm':float('Inf')}\n",
    "        param_multi_ker_rbf = {'epsilon': epsilon_vec[i], 'kappa': kappa_vec[j], 'pnorm':float('Inf'),\n",
    "                                  'kernel': 'rbf', 'gamma': 'Auto'}\n",
    "        \n",
    "        param_ova = {'epsilon': epsilon_vec[i], 'kappa': kappa_vec[j], 'pnorm':float('Inf')}\n",
    "        param_ova_ker_rbf = {'epsilon': epsilon_vec[i], 'kappa': kappa_vec[j], 'pnorm':float('Inf'),\n",
    "                                'kernel': 'rbf', 'gamma': 'Auto'}\n",
    "        \n",
    "        classifier_multi = DR_MSVM(param_multi)\n",
    "        optimal_multi = classifier_multi.train(train_data_multi)\n",
    "        y_pred_multi = classifier_multi.test(x_test)\n",
    "        acc_multi = classifier_multi.evaluate_accuracy(y_test_multi,y_pred_multi)\n",
    "        acc_tensor_multi[i,j,0] = acc_multi\n",
    "        \n",
    "        classifier_multi = DR_MSVM(param_multi)\n",
    "        optimal_multi = classifier_multi.train(train_data_multi_s)\n",
    "        y_pred_multi = classifier_multi.test(x_test)\n",
    "        acc_multi = classifier_multi.evaluate_accuracy(y_test_multi,y_pred_multi)\n",
    "        acc_tensor_multi[i,j,1] = acc_multi\n",
    "        \n",
    "        classifier_multi_ker_rbf = kDR_MSVM(param_multi_ker_rbf)\n",
    "        optimal_multi_ker_rbf = classifier_multi_ker_rbf.train(train_data_multi)\n",
    "        y_pred_multi_ker_rbf = classifier_multi_ker_rbf.test(x_test,train_data_multi)\n",
    "        acc_multi_ker_rbf = classifier_multi_ker_rbf.evaluate_accuracy(y_test_multi,y_pred_multi_ker_rbf)\n",
    "        acc_tensor_multi_ker_rbf[i,j,0] = acc_multi_ker_rbf\n",
    "        \n",
    "        classifier_multi_ker_rbf = kDR_MSVM(param_multi_ker_rbf)\n",
    "        optimal_multi_ker_rbf = classifier_multi_ker_rbf.train(train_data_multi_s)\n",
    "        y_pred_multi_ker_rbf = classifier_multi_ker_rbf.test(x_test,train_data_multi)\n",
    "        acc_multi_ker_rbf = classifier_multi_ker_rbf.evaluate_accuracy(y_test_multi,y_pred_multi_ker_rbf)\n",
    "        acc_tensor_multi_ker_rbf[i,j,1] = acc_multi_ker_rbf\n",
    "        \n",
    "        classifier_ova = DR_OVA(param_ova)\n",
    "        optimal_ova = classifier_ova.train(train_data_ova)\n",
    "        y_pred_ova = classifier_ova.test(x_test)\n",
    "        acc_ova = classifier_ova.evaluate_accuracy(y_test,y_pred_ova)\n",
    "        acc_tensor_ova[i,j,0] = acc_ova\n",
    "        \n",
    "        classifier_ova = DR_OVA(param_ova)\n",
    "        optimal_ova = classifier_ova.train(train_data_ova_s)\n",
    "        y_pred_ova = classifier_ova.test(x_test)\n",
    "        acc_ova = classifier_ova.evaluate_accuracy(y_test,y_pred_ova)\n",
    "        acc_tensor_ova[i,j,1] = acc_ova\n",
    "        \n",
    "        classifier_ova_ker_rbf = kDR_OVA(param_ova_ker_rbf)\n",
    "        optimal_ova_ker_rbf = classifier_ova_ker_rbf.train(train_data_ova)\n",
    "        y_pred_ova_ker_rbf = classifier_ova_ker_rbf.test(x_test,train_data_ova)\n",
    "        acc_ova_ker_rbf = classifier_ova_ker_rbf.evaluate_accuracy(y_test,y_pred_ova_ker_rbf)\n",
    "        acc_tensor_ova_ker_rbf[i,j,0] = acc_ova_ker_rbf\n",
    "        \n",
    "        classifier_ova_ker_rbf = kDR_OVA(param_ova_ker_rbf)\n",
    "        optimal_ova_ker_rbf = classifier_ova_ker_rbf.train(train_data_ova_s)\n",
    "        y_pred_ova_ker_rbf = classifier_ova_ker_rbf.test(x_test,train_data_ova)\n",
    "        acc_ova_ker_rbf = classifier_ova_ker_rbf.evaluate_accuracy(y_test,y_pred_ova_ker_rbf)\n",
    "        acc_tensor_ova_ker_rbf[i,j,1] = acc_ova_ker_rbf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c4911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
